<!doctype html><html lang=en style=height:100%><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Shared Tasks (2020)</title><meta name=generator content="Hugo 0.101.0"><link href=/anthology/favicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/anthology/css/main.min.bc64f42696ede127a6ae3a3d07b3d03939e143e52b5450fd4a2d07d3a8fad2e6.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body style=display:flex;flex-flow:column;height:100%><nav class="navbar navbar-expand-sm navbar-light bg-light bg-gradient-light shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container-fluid style=padding-top:15px;padding-bottom:15px><a class=navbar-brand href=/anthology/><span class=d-md-inline><span style=color:#951515><b>IR</b></span> Anthology</span></a><form class="form-inline input-group" style=max-width:800px action=https://ir.chatnoir.eu/? method=get><input id=acl-search-box class=form-control name=q type=search placeholder="Full-text search..." aria-label=Search><div class=input-group-append><button class="btn btn-outline-primary" type=submit><i class="fas fa-search"></i></button></div></form></div></nav><div id=main-container class=container-fluid><section id=main class=col-12><small><a href=/anthology>Main</a> »</small><br><h2 id=title>Shared Tasks (2020)</h2><hr><div id=content-panel class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Content</h4><ul class=list-pl-responsive><li><a class=align-middle href=#CLEF>CLEF</a><br><ul><li><a class=align-right href=#ARQMath>ARQMath</a></li></ul><ul><li><a class=align-right href=#BioASQ>BioASQ</a></li></ul><ul><li><a class=align-right href=#ChEMU>ChEMU</a></li></ul><ul><li><a class=align-right href=#CheckThat%21>CheckThat!</a></li></ul><ul><li><a class=align-right href=#HIPE>HIPE</a></li></ul><ul><li><a class=align-right href=#ImageCLEF>ImageCLEF</a></li></ul><ul><li><a class=align-right href=#LiLAS>LiLAS</a></li></ul><ul><li><a class=align-right href=#LifeCLEF>LifeCLEF</a></li></ul><ul><li><a class=align-right href=#PAN>PAN</a></li></ul><ul><li><a class=align-right href=#Touch%20>Touch</a></li></ul><ul><li><a class=align-right href=#eHealth>eHealth</a></li></ul><ul><li><a class=align-right href=#eRisk>eRisk</a></li></ul></li><li><a class=align-middle href=#FIRE>FIRE</a><br><ul><li><a class=align-right href=#Anaphora%20Resolution%20from%20Social%20Media%20Text%20in%20Indian%20Languages%20%28SocAnaRes-IL%29%0d%0a>Anaphora Resolution from Social Media Text in Indian Languages (SocAnaRes-IL)</a></li></ul><ul><li><a class=align-right href=#Artificial%20Intelligence%20for%20Legal%20Assistance%20%28AILA%29>Artificial Intelligence for Legal Assistance (AILA)</a></li></ul><ul><li><a class=align-right href=#Authorship%20Identification%20of%20Source%20Code%20%28AI-SOCO%29>Authorship Identification of Source Code (AI-SOCO)</a></li></ul><ul><li><a class=align-right href=#CEREX%20%28Cause-Effect%20Relation%20EXtraction%20from%20Text%29>CEREX (Cause-Effect Relation EXtraction from Text)</a></li></ul><ul><li><a class=align-right href=#Causality-driven%20Ad%20hoc%20Information%20Retrieval%20%28CAIR%29>Causality-driven Ad hoc Information Retrieval (CAIR)</a></li></ul><ul><li><a class=align-right href=#Dravidian%20CodeMix%20Hate%20Speech%20and%20Offensive%20Content%20Identification%20in%20Indo-European%20Languages%20%28Dravdian-CodeMix-HASOC%29>Dravidian CodeMix Hate Speech and Offensive Content Identification in Indo-European Languages (Dravdian-CodeMix-HASOC)</a></li></ul><ul><li><a class=align-right href=#Event%20Detection%20from%20News%20in%20Indian%20Languages%20%28EDNIL%29>Event Detection from News in Indian Languages (EDNIL)</a></li></ul><ul><li><a class=align-right href=#Fake%20News%20Detection%20in%20the%20Urdu%20Language%20%28UrduFake%29%0d%0a>Fake News Detection in the Urdu Language (UrduFake)</a></li></ul><ul><li><a class=align-right href=#Hate%20Speech%20and%20Offensive%20Content%20Identification%20in%20Indo-European%20Languages%20%28HASOC%29>Hate Speech and Offensive Content Identification in Indo-European Languages (HASOC)</a></li></ul><ul><li><a class=align-right href=#Retrieval%20from%20Conversational%20Dialogues%20%28RCD%29>Retrieval from Conversational Dialogues (RCD)</a></li></ul></li><li><a class=align-middle href=#TREC>TREC</a><br><ul><li><a class=align-right href=#Conversational%20Assistance%20%28CAsT%29>Conversational Assistance (CAsT)</a></li></ul><ul><li><a class=align-right href=#Deep%20Learning>Deep Learning</a></li></ul><ul><li><a class=align-right href=#Fair%20Ranking>Fair Ranking</a></li></ul><ul><li><a class=align-right href=#Health%20Misinformation>Health Misinformation</a></li></ul><ul><li><a class=align-right href=#Incident%20Streams>Incident Streams</a></li></ul><ul><li><a class=align-right href=#News>News</a></li></ul><ul><li><a class=align-right href=#Podcasts>Podcasts</a></li></ul><ul><li><a class=align-right href=#Precision%20Medicine>Precision Medicine</a></li></ul></li></ul></div></div><div id=CLEF><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a style=text-align:center><a class=align-middle href=/anthology/venues/clef_conference>CLEF<br><br></a></a></h4><div id=ARQMath><h4><a>ARQMath</a></h4><ul><li><a>Find Answers</a><p>Given a posted question as a query, search all answer posts and return relevant answer posts.</p></li></ul><ul><li><a>Formula Search</a><p>Given a question post with an identified formula as a query, search all question and answer posts and return relevant formulas with their posts.</p></li></ul></div><div id=BioASQ><h4><a>BioASQ</a></h4><ul><li><a>BioASQ Task 10a</a><p>This task will be based on the standard process followed by PubMed to index journal abstracts. The participants will be asked to classify new PubMed documents, written in English, as they become available online, before PubMed curators annotate (in effect, classify) them manually. The classes will come from the MeSH hierarchy; they will be the subject headings that are currently used to manually index the abstracts, excluding those that are already provided by the authors of each article. As new manual annotations become available, they will be used to evaluate the classification performance of participating systems (that classify articles before they are manually annotated), using standard IR measures (e.g., precision, recall, accuracy), as well as hierarchical variants of them. The participants will be able to train their classifiers, using the whole history of manually annotated abstracts.</p></li></ul><ul><li><a>BioASQ Task 10b</a><p>Task 10b will use benchmark datasets containing training and test biomedical questions, in English, along with gold standard (reference) answers. The participants will have to respond to each test question with relevant concepts (from designated terminologies and ontologies), relevant articles (in English, from designated article repositories), relevant snippets (from the relevant articles), relevant RDF triples (from designated ontologies), exact answers (e.g., named entities in the case of factoid questions) and 'ideal' answers (English paragraph-sized summaries). More than 4,200 training questions (that were used as dry-run or test questions in previous year) are already available, along with their gold standard answers (relevant concepts, articles, snippets, exact answers, summaries). About 500 new test questions will be used this year. All the questions are constructed by biomedical experts from around Europe.</p></li></ul><ul><li><a>BioASQ Task DisTEMIST</a><p>The novel DisTEMIST task will focus on the recognition and indexing of diseases in medical documents, by posing subtasks on (1) indexing medical documents with controlled terminologies (2) automatic detection indexing textual evidence, i.e. disease entity mentions in text and (3) normalization of these disease mentions to terminologies.</p></li></ul><ul><li><a>BioASQ Task Synergy</a><p>Task Synergy will use benchmark datasets of test biomedical questions for COVID-19, in English. The participants will have to respond to each test question with relevant articles (in English, from designated article repositories), relevant snippets (from the relevant articles), exact answers (e.g., named entities in the case of factoid questions) and 'ideal' answers (English paragraph-sized summaries). No special training questions are available for this task, but expert feedback will be incrementally provided instead, based on participant responses for each round. Using this feedback, the participants can improve their systems and provide better answers for persisting and/or new questions. Meanwhile, the participants may also trainin their systems using the training dataset from previous versions of task Synergy and of task 10b as well, both available at the BioASQ Participants Area . All the questions are constructed and assessed by biomedical experts from around Europe. Participation in the task can be partial, i.e. participants may enter the task in any of the rounds.</p></li></ul></div><div id=ChEMU><h4><a>ChEMU</a></h4><ul><li><a>Task 1</a><p>Named Entity Recognition involves identifying chemical compounds as well as their types in context, i.e., to assign the label of a chemical compound according to the role which the compound plays within a chemical reaction.</p></li></ul><ul><li><a>Task 2</a><p>Event extraction over chemical reactions involves event trigger detection and argument recognition.</p></li></ul></div><div id=CheckThat!><h4><a>CheckThat!</a></h4><ul><li><a>Task 1</a><p>Predict which tweet from a stream of tweets on a topic should be prioritized for fact-checking.</p></li></ul><ul><li><a>Task 2</a><p>Given a check-worthy tweet claim, and a set of previously-checked claims, determine whether the claim has been already fact-checked.</p></li></ul><ul><li><a>Task 4</a><p>Given a check-worthy claim in a tweet and a set of potentially-relevant Web pages, estimate the veracity of the claim.</p></li></ul><ul><li><a>Task 5</a><p>Given a debate segmented into sentences, together with speaker information, prioritize sentences for fact-checking.</p></li></ul><ul><li><a>Task3</a><p>Given a check-worthy claim on a specific topic and a set of text snippets extracted from potentially-relevant webpages, return a ranked list of evidence snippets for the claim.</p></li></ul></div><div id=HIPE><h4><a>HIPE</a></h4><ul><li><a>Task 1</a><p>Subtask 1.1: NERC Coarse-grained: this task includes the recognition and classification of entity mentions according to coarse-grained types (Person, Location, Organisation and Product). Subtask 1.2: NERC Fine-grained: this task includes the recognition and classification of entity mentions according to fine-grained types (cf. column 2 in Table 2), plus the detection and classification of nested entities of depth 1, as well as entity mention components.</p></li></ul><ul><li><a>Task 2</a><p>This task includes the linking of named entity mentions to a unique referent in a knowledge base (KB) or to a NIL node if the mention does not have a referent in the KB. The chosen KB is Wikidata. The entity linking task includes two settings: with and without prior knowledge of mention boundaries. Concretely speaking, the evaluation period will consist of two consecutive rounds, where a first NEL task without prior information on mentions will be evaluated during round 1 (i.e. task bundles 1 and 2), and a second one with information on mention boundaries (but no type) during the second round (bundle 5).</p></li></ul></div><div id=ImageCLEF><h4><a>ImageCLEF</a></h4><ul><li><a>ImageCLEFcoral</a><p>The increasing use of structure-from-motion photogrammetry for modelling large-scale environments from action cameras has driven the next generation of visualization techniques. The task addresses the problem of automatically segmenting and labeling a collection of images that can be used in combination to create 3D models for the monitoring of coral reefs.</p></li></ul><ul><li><a>ImageCLEFdrawnUI</a><p>Enabling people to create websites by drawing them on a piece of paper can make the webpage building process more accessible. The task addresses the problem of automatically recognizing hand drawn objects representing website UIs, that will be further translated into automatic website code.</p></li></ul><ul><li><a>ImageCLEFlifelog</a><p>An increasingly wide range of personal devices that allow capturing pictures, videos, and audio clips for every moment of our lives, are becoming available. In this context, the task addresses the problems of lifelogging data retrieval and summarization.</p></li></ul><ul><li><a>ImageCLEFmedical</a><p>Medical images can be used in a variety of scenarios and this task will combine the most popular medical tasks of ImageCLEF and continue the last year idea of combining various applications, namely: automatic image captioning and scene understanding, medical visual question answering and decision support on tuberculosis. This allows to explore synergies between the tasks.</p></li></ul></div><div id=LiLAS><h4><a>LiLAS</a></h4><ul><li><a>Task 1</a><p>Given a query, find the most relevant documents!</p></li></ul><ul><li><a>Task 2</a><p>Given a seed publication, recommend research datasets</p></li></ul></div><div id=LifeCLEF><h4><a>LifeCLEF</a></h4><ul><li><a>BirdCLEF</a><p>Two scenarios will be evaluated: (i) the recognition of all specimens singing in a long sequence (up to one hour) of raw soundscapes that can contain tens of birds singing simultaneously, and (ii) chorus source separation in complex soundscapes that were recorded in stereo at very high sampling rate (250 kHz SR). For the first scenario, participants will be asked to provide time intervals of recognized singing birds. Participants will be allowed to use any of the provided metadata complementary to the audio content (wav format, 44.1 kHz, 48 kHz, or 96 kHz sampling rate). The task is focused on developing real-world applicable solutions and therefore requires participants to submit single models trained on none other than the mono-species recordings provided as training data. For the second task on stereophonic recordings, the goal will be to determine the species singing in chorus simultaneously during a time interval. In contrast to task one, the challengers are invited to run automatic source separation before or jointly to the bird species classification, taking advantage of the multi-channel recordings. Participants will be allowed to use any other data than the provided recordings, but will have to provide the scripts to check that their solution is fully automatic. For both tasks, the evaluation measure will be the classification mean Average Precision</p></li></ul><ul><li><a>GeoLifeCLEF</a><p>The detailed description of the challenge is provided on the AICrowd page of the challenge: GeoLifeCLEF 2020 . In a nutshell, the occurrence dataset is split in a training set with known species name labels and a test set used for the evaluation. For each occurrence in the test set (paired with the corresponding satellite image and environmental co-variates), the goal of the task will is to return a candidate set of species with associated confidence scores. The evaluation metric will be an adaptive top-K accuracy.</p></li></ul><ul><li><a>PlantCLEF</a><p>The challenge will be evaluated as a cross-domain classification task. The training set will consist of herbarium sheets whereas the test set will be composed of field pictures. To enable learning a mapping between the herbarium sheets domain and the field pictures domain, we will provide both herbarium sheets and field pictures for a subset of species. The metrics used for the evaluation of the task will be the classification accuracy and the Mean Reciprocal Rank.</p></li></ul><ul><li><a>SnakeCLEF</a><p>Given the set of images and corresponding geographic locality information, the goal of the task will be to return for each image a ranked list of species sorted according to the likelihood that they are in the image and might have been observed at that location.</p></li></ul></div><div id=PAN><h4><a>PAN</a></h4><ul><li><a>Authorship Verification</a><p>Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles. Given a large training dataset comprising of known authors who have written about a given set of topics, the test dataset contains verification cases from a subset of the authors and topics found in the training data.</p></li></ul></div><div id="Touch "><h4><a>Touch</a></h4><ul><li><a>Task 1</a><p>The goal of Task 1 is to support users who search for arguments to be used in conversations (e.g., getting an overview of pros and cons or just looking for arguments in line with a user's stance). Given a question on a controversial topic, the task is to retrieve relevant arguments from a focused crawl of online debate portals.</p></li></ul><ul><li><a>Task 2</a><p>The goal of Task 2 is to support users facing some choice problem from "everyday life". Given a comparative question, the task is to retrieve and rank documents from the ClueWeb12 that help to answer the comparative question.</p></li></ul></div><div id=eHealth><h4><a>eHealth</a></h4><ul><li><a>Task 1</a><p>This year s Multilingual Information Extraction (IE) task continues the growth path identified in previous year s CLEF eHealth IE challenges. The 2020 task focuses on ICD coding for clinical textual data in Spanish. Coding also includes some textual evidence annotations. A subtask focuses on term mapping, which is now a crucial and pressing need. The terms considered are extracted from EHRs in Spanish and they are manually linked to HPO, ICD10 and SNOMED CT. These tasks can be treated as a named entity recognition and normalisation task, but also as a text classification task. This year the lab proposes 3 subtasks:
ICD10-CM [CIE10 Diagn stico] codes assignment. This sub-track evaluates systems that predict ICD10-CM codes (in the Spanish translation, CIE10-Diagn stico codes).
ICD10-PCS [CIE10 Procedimiento] codes assignment. This sub-track evaluates systems that predict ICD10-PCS codes (in the Spanish translation, CIE10-Procedimiento codes).
Explainable AI. Systems are required to submit the reference to the predicted codes (both ICD10-CM and ICD10-PCS). The correctness of the provided reference is assessed in this sub-track, in addition to the code prediction.</p></li></ul><ul><li><a>Task 2</a><p>The 2020 CLEF eHealth Task 2 on consumer health search builds on the information retrieval tasks that have run at CLEF eHealth since its inception. The consumer health search task follows a standard information retrieval shared challenge paradigm from the perspective that it provides challenge participants with a test collection consisting of a set of documents and a set of topics to develop retrieval techniques for. Runs submitted by participants are pooled, and manual relevance assessment conducted.
This year the lab proposes 2 subtasks:
Adhoc subtask
Spoken queries subtask</p></li></ul></div><div id=eRisk><h4><a>eRisk</a></h4><ul><li><a>Task 1</a><p>The challenge consists in performing a task on early risk detection of self-harm. The challenge consists of sequentially processing pieces of evidence and detect early traces of self-harm as soon as possible. The task is mainly concerned about evaluating Text Mining solutions and, thus, it concentrates on texts written in Social Media. Texts should be processed in the order they were created. In this way, systems that effectively perform this task could be applied to sequentially monitor user interactions in blogs, social networks, or other types of online media.</p></li></ul><ul><li><a>Task 2</a><p>This is a continuation of eRisk 2019's T3 task. The task consists of estimating the level of depression from a thread of user submissions. For each user, the participants will be given a history of postings and the participants will have to fill a standard depression questionnaire (based on the evidence found in the history of postings).</p></li></ul></div></div><div id=FIRE><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a style=text-align:center><a class=align-middle href=/anthology/venues/fire_conference>FIRE<br><br></a></a></h4><div id="Anaphora Resolution from Social Media Text in Indian Languages (SocAnaRes-IL)
"><h4><a>Anaphora Resolution from Social Media Text in Indian Languages (SocAnaRes-IL)</a></h4><ul><li><a>Task</a><p>The task is to identify Anaphor and its antecedent in a given text. The text is a tweet.</p></li></ul></div><div id="Artificial Intelligence for Legal Assistance (AILA)"><h4><a>Artificial Intelligence for Legal Assistance (AILA)</a></h4><ul><li><a>Task 1</a><p>Given a query (description of a situation), identify relevant statutes and prior-cases</p></li></ul><ul><li><a>Task 2</a><p>The task is to semantically segment a legal case document. More formally, it is a sentence classification task, where each sentence has to be assigned one of the 7 predefined labels or "rhetorical roles".</p></li></ul></div><div id="Authorship Identification of Source Code (AI-SOCO)"><h4><a>Authorship Identification of Source Code (AI-SOCO)</a></h4><ul><li><a>A</a><p>Capitalization is writing a word with its first letter as a capital letter. Your task is to capitalize the given word. Input: A single line contains a non-empty word. This word consists of lowercase and uppercase English letters. The length of the word will not exceed 10^3. Output: The given word after capitalization.</p></li></ul></div><div id="CEREX (Cause-Effect Relation EXtraction from Text)"><h4><a>CEREX (Cause-Effect Relation EXtraction from Text)</a></h4><ul><li><a>Task A</a><p>Identify whether a given sentence contains a causal event (either cause/effect).</p></li></ul><ul><li><a>Task B</a><p>Annotate each word in a sentence in terms of the four labels cause (C),effect(E), causal connectives(CC) and None.</p></li></ul></div><div id="Causality-driven Ad hoc Information Retrieval (CAIR)"><h4><a>Causality-driven Ad hoc Information Retrieval (CAIR)</a></h4><ul><li><a>Task</a><p>Participants will be given a static test collection of documents and a list of queries related to events that are likely to be caused by a number of other past events. The participants are then required to develop ranking models that can effectively retrieve documents containing information on such past events that are likely candidates to lead to the query event. The officially submitted ranked lists of different participating systems will then be evaluated by comparing them against a set of manually judged relevant documents.</p></li></ul></div><div id="Dravidian CodeMix Hate Speech and Offensive Content Identification in Indo-European Languages (Dravdian-CodeMix-HASOC)"><h4><a>Dravidian CodeMix Hate Speech and Offensive Content Identification in Indo-European Languages (Dravdian-CodeMix-HASOC)</a></h4><ul><li><a>Task-1</a><p>People use offensive content in their social media posts to degrade an individual or religion or other organizations in many respects. The identification of such social media posts is a necessity. A substantial amount of work has been done in languages like English. However, the offensive language identification in Indian language scenario is still an unexplored area. One of the key reasons is the code-mixing.
The goal of this task is to identify the offensive language of the code-mixed dataset of comments/posts in Dravidian Languages (Tamil-English and Malayalam-English and ) collected from social media. Each comment/post is annotated with offensive language label at the comment/post level. The data set has been collected from YouTube comments and Tweets.
This is a message-level label classification task. Given a YouTube comment in Code-mixed (Mixture of Native and Roman Script) Tamil and Malayalam, systems have to classify it into offensive or not-offensive.</p></li></ul><ul><li><a>Taskl-2</a><p>People use offensive content in their social media posts to degrade an individual or religion or other organizations in many respects. The identification of such social media posts is a necessity. A substantial amount of work has been done in languages like English. However, the offensive language identification in Indian language scenario is still an unexplored area. One of the key reasons is the code-mixing.
The goal of this task is to identify the offensive language of the code-mixed dataset of comments/posts in Dravidian Languages (Tamil-English and Malayalam-English and ) collected from social media. Each comment/post is annotated with offensive language label at the comment/post level. The data set has been collected from YouTube comments and Tweets.
This is a message-level label classification task. Given a tweet or YouTube comments in Tanglish and Manglish (Tamil and Malayalam using written using Roman Script), systems have to classify it into offensive or not-offensive.</p></li></ul></div><div id="Event Detection from News in Indian Languages (EDNIL)"><h4><a>Event Detection from News in Indian Languages (EDNIL)</a></h4><ul><li><a>Task 1</a><p>Event Identification: Identify a piece of text from news articles that contain an event.</p></li></ul><ul><li><a>Task 2</a><p>Event Frame: Create an event frame from the news article containing the following details:
Type: Type and subtype of the line containing the event
Casualties: No of people is injured or killed/Damages to properties
Time: When the event takes place
Place: Where the event takes place
Reason: Why and how the event takes place</p></li></ul></div><div id="Fake News Detection in the Urdu Language (UrduFake)
"><h4><a>Fake News Detection in the Urdu Language (UrduFake)</a></h4><ul><li><a>Task</a><p>The dissemination of Fake news always beat out the truth with significant growth. Fake news and false rumors are spreading further and faster, reaching more people, and penetrating deeper into social networks. We propose the task titled Fake News Detection in the Urdu Language", which aims at identifying deceiving news articles in the Urdu language spread via digital media. Urdu fake news detection has been investigated (Amjad al.,2020), and we want better results at the level of English language and more methods. The objective of organizing this task is to address the problem of detecting deceiving information in Urdu language using digital media text.</p></li></ul></div><div id="Hate Speech and Offensive Content Identification in Indo-European Languages (HASOC)"><h4><a>Hate Speech and Offensive Content Identification in Indo-European Languages (HASOC)</a></h4><ul><li><a>Sub-task A</a><p>Sub-task A focus on Hate speech and Offensive language identification offered for English, German, Hindi. Sub-task A is coarse-grained binary classification in which participating system are required to classify tweets into two class, namely: Hate and Offensive (HOF) and Non- Hate and offensive (NOT).
(NOT) Non Hate-Offensive - This post does not contain any Hate speech, profane, offensive content.
(HOF) Hate and Offensive - This post contains Hate, offensive, and profane content.</p></li></ul><ul><li><a>Sub-task B</a><p>This sub-task is a fine-grained classification offered for English, German, Hindi.. Hate-speech and offensive posts from the sub-task A are further classified into three categories.
(HATE) Hate speech :- Posts under this class contain Hate speech content.
(OFFN) Offenive :- Posts under this class contain offensive content.
(PRFN) Profane :- These posts contain profane words.</p></li></ul></div><div id="Retrieval from Conversational Dialogues (RCD)"><h4><a>Retrieval from Conversational Dialogues (RCD)</a></h4><ul><li><a>Task 1</a><p>Given an excerpt of a dialogue act as shown in example above, output the span of text indicating a potential piece of information need (requiring contextualization), i.e., in case of the above example, output the text Fifth Amendment.</p></li></ul><ul><li><a>Task 2</a><p>Given an excerpt of a dialogue act (see above example), return a ranked list of passages containing information on the topic of the information need (requiring contextualization), i.e., with respect to the above example return passages from Wikipedia that contain information on the Fifth Amendment.</p></li></ul></div></div><div id=TREC><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a style=text-align:center><a class=align-middle href=/anthology/venues/trec_conference>TREC<br><br></a></a></h4><div id="Conversational Assistance (CAsT)"><h4><a>Conversational Assistance (CAsT)</a></h4><ul><li><a>Task</a><p>The core of the CAsT 2020 task is unchanged from CAsT 2019. The goal of the task is to satisfy a user s complex information need expressed through multi-turn conversational queries/utterances (?) for each turn ? = {?1, ...?? ...??}, by retrieving and ranking passages from MS MARCO [1] and Wikipedia the Complex Answer Retrieval (CAR) corpora [4]. The CAsT 2019 overview and dataset paper provide detail about the this setup [2, 3].</p></li></ul></div><div id="Deep Learning"><h4><a>Deep Learning</a></h4><ul><li><a>Task 1</a><p>The first task focuses on document retrieval, with two subtasks: (i) Full retrieval and (ii) top-100 reranking. In the full retrieval subtask, the runs are expected to rank documents based on their relevance to the query, where documents can be retrieved from the full document collection provided. This subtask models the end-to-end retrieval scenario. In the reranking subtask, participants were provided with an initial ranking of 100 documents, giving all participants the same starting point. This is a common scenario in many real-world retrieval systems that employ a telescoping architecture [Matveeva et al., 2006, Wang et al., 2011]. The reranking subtask allows participants to focus on learning an effective relevance estimator, without the need for implementing an end-to-end retrieval system. It also makes the reranking runs more comparable, because they all rerank the same set of 100 candidates. The initial top-100 rankings were retrieved using Indri [Strohman et al., 2005] on the full corpus with Krovetz stemming and stopwords eliminated. Judgments are on a four-point scale: [3] Perfectly relevant: Document is dedicated to the query, it is worthy of being a top result in a search engine. [2] Highly relevant: The content of this document provides substantial information on the query. [1] Relevant: Document provides some information relevant to the query, which may be minimal. [0] Irrelevant: Document does not provide any useful information about the query. For metrics that binarize the judgment scale, we map document judgment levels 3,2,1 to relevant and map document judgment level 0 to irrelevant.</p></li></ul><ul><li><a>Task 2</a><p>Similar to the document retrieval task, the passage retrieval task includes (i) a full retrieval and (ii) a top-1000 reranking tasks. In the full retrieval subtask, given a query, the participants were expected to retrieve a ranked list of passages from the full collection based on their estimated likelihood of containing an answer to the question. Participants could submit up to 1000 passages per query for this end-to-end retrieval task. In the top-1000 reranking subtask, 1000 passages per query were provided to participants, giving all participants the same starting point. The sets of 1000 were generated based on BM25 retrieval with no stemming as applied to the full collection. Participants were expected to rerank the 1000 passages based on their estimated likelihood of containing an answer to the query. In this subtask, we can compare different reranking methods based on the same initial set of 1000 candidates, with the same rationale as described for the document reranking subtask. Judgments are on a four-point scale: [3] Perfectly relevant: The passage is dedicated to the query and contains the exact answer. [2] Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information. [1] Related: The passage seems related to the query but does not answer it. [0] Irrelevant: The passage has nothing to do with the query. For metrics that binarize the judgment scale, we map passage judgment levels 3,2 to relevant and map document judgment levels 1,0 to irrelevant.</p></li></ul></div><div id="Fair Ranking"><h4><a>Fair Ranking</a></h4><ul><li><a>Task 1</a><p>For 2020, we again adopted an academic search task, where we have a corpus of academic article abstracts and queries submitted to a production academic search engine. The central goal of the Fair Ranking track is to provide fair exposure to different groups of authors (a group fairness framing). We recognize that there may be multiple group definitions (e.g. based on demographics, stature, topic) and hoped for the systems to be robust to these. We expected participants to develop systems that optimize for fairness and relevance for arbitrary group definitions, and did not reveal the exact group definitions until after the evaluation runs were submitted. "Rerank" runs sorted a query-dependent list of documents to simultaneously provide fairness and relevance.</p></li></ul><ul><li><a>Task 2</a><p>For 2020, we again adopted an academic search task, where we have a corpus of academic article abstracts and queries submitted to a production academic search engine. The central goal of the Fair Ranking track is to provide fair exposure to different groups of authors (a group fairness framing). We recognize that there may be multiple group definitions (e.g. based on demographics, stature, topic) and hoped for the systems to be robust to these. We expected participants to develop systems that optimize for fairness and relevance for arbitrary group definitions, and did not reveal the exact group definitions until after the evaluation runs were submitted. "Retrieval" runs returned 100-item rankings from the corpus in response to a query string.</p></li></ul></div><div id="Health Misinformation"><h4><a>Health Misinformation</a></h4><ul><li><a>Task 1</a><p>For the total recall task, the goal is to identify all the documents conveying incorrect information for a specific set of topics. Documents contradicting the topic s answer are assumed to be misinformation. Participants were to identify all documents in a collection that promulgate, promote, and/or support that misinformation. For example, for the topic Can Ibuprofen worsen COVID19 , participants were to identify all documents indicating that Ibuprofen can worsen COVID-19. Documents making this claim for the purposes of debunking it are not misinformation. Participants submitted runs that ranked documents according to the likelihood that they promulgate misinformation. Up to 10,000 documents per topic could be submitted.</p></li></ul><ul><li><a>Task 2</a><p>For the ad-hoc retrieval task, the goal is to design a ranking model that promotes credible and correct information over incorrect information. For a given topic, participants were to return relevant, credible, and correct information that will help searchers make correct decisions. Participants were to assume that the statement included in the topic description is correct or not, based on the answer field, even if they knew current medical or other evidence suggests otherwise. Runs were allowed to contain a maximum of 1,000 documents per topic. Note that this task is more than simply a new definition of what is relevant. There are multiple types of results: useful and correct and credible, useful and correct but not credible, etc. as well as incorrect and non-useful documents. It is important that search results avoid containing incorrect results. In place of notions of correctness, the credibility of the information source is useful, and useful and credible information is preferred.</p></li></ul></div><div id="Incident Streams"><h4><a>Incident Streams</a></h4><ul><li><a>Task 1</a><p>This task is an extension of the core TREC-IS task used in 2018 and 2019. Systems participating in this task are given tweet streams from a collection of crisis events and should classify each tweet as having one or more of the high-level information types described in the ontology section above. Each tweet should be assigned as many categories as are appropriate. Likewise, each message must be tagged with a numeric measure of priority, on a [0, 1] scale (in 2020-A, we used an ordinal value of Low , Medium , High , or Critical but returned to the numeric scale after discussions with participants)</p></li></ul><ul><li><a>Task 2</a><p>(Introduced in 2020-A) A common concern in Task 1 is that the number of high-level information types makes it difficult to dive deeply into the labels. Obtaining a deeper understanding of these labels appears key to a high-performing system, however, as systems with strong feature engineering have performed highly in previous TREC-IS editions. To address this issue, TREC-IS 2020 now includes a restricted version of Task 1 that focuses only on 11 of the high-level information types. These 11 include the top six information types labeled as actionable in previous editions (i.e., the types that have, on average, the highest priority) as well as five other types selected from the full set used in Task 1, as shown in Table 1 (marked by a ).</p></li></ul><ul><li><a>Task 3</a><p>(Introduced in 2020-A) In this task, systems are intended to provide public health officials and emergency response officers with additional tooling and evaluation data for future public health emergencies or resurgence of COVID-19. COVID-19 is different from prior events, however, in that its impacts are global in scale and are difficult to restrict to a single region as we can with wildfires or earthquakes. It is therefore difficult to identify primary affected locations (whereas hurricanes or wildfires are geographically constrained). Hence, for COVID-19 TREC-IS focuses data collections around regional areas of interest, primarily population centers. In 2020-A, this task restricted the set of information types to a subset of eight relevant types (see Table 1 for types marked with a ) as the track lacked training data for public health events. For 2020-B, however, we have removed the category restriction as analyses of 2020-A COVID-19 data has found instances across 24 of the 25 information types (weather-related content was not found). Consequently, this Task 3 parallels Task 1 and differs only in the data against which participant systems are evaluated.</p></li></ul></div><div id=News><h4><a>News</a></h4><ul><li><a>Background Linking</a><p>The goal of the background linking task is to develop evaluation data to support researchers in developing systems that can help users contextualize news articles as they are reading them. News websites nearly always link to related articles in a sidebar, at the end of an article, from within the text of the article, or all three. We want to look at a particular case for linking: given that the user is reading a specific article (the query article), algorithms should recommend articles that this person should read next that are the most useful for providing context and background for the query article. The background article can be dated before or after the query article, because we are considering the use case where the user is reading the query article now, irrespective of when it was published, and the system is recommending background reading live at the time when the user is reading the query article.</p></li></ul><ul><li><a>Wikification</a><p>In addition to providing links to articles that give the reader background or contextual information, journalists sometimes link mentions of concepts, artifacts, and entities to internal or external pages with in depth information that will help the reader better understand the article. In the first two years of the track, we worked on ranking entities within the document as to their priority for linking. For this year, the task was grown into a full wikification task. Wikification is providing Wikipedia-like markup in a document by linking related content to contextually-relevant anchors. The goal of wikification depends on the application; in Wikipedia, the goal is to link articles to other articles as cross-references. For news reading, we imagine that wikification could be used to recommend interstitial links to the author or editor of the article, where those links serve a similar function to results in the background linking task: provide essential background and context to the reader of the article. This definition of wikification differs in some important ways from the natural language processing field s version, where it essentially means linking entities in the article (which may be a Wikipedia article) to other Wikipedia articles. First, we do not restrict the anchor to entity mentions but rather they can be any contiguous span of text within a block. Second, the target of the link can be the knowledge base (Wikipedia) or another article in the Post. Lastly, the task is formulated as ranked recommendation, so high performance is modeled by selecting useful anchors and linking them in a way that provides context to the user, instead of complete and correct entity linking. The task is operationalized as follows: Given the same starting article as in the background linking task, systems were to provide a ranked list of link anchors and suggestions. A link anchor was a triple of content block number, starting 6 character, and anchor length, defining an extent of characters in the document. The suggested target for the anchor could be either another Washington Post article, or a Wikipedia article in the provided Wikipedia dump. Individual (anchor, target) pairs are ranked by the system-provided score as in a traditional TREC run. At the time that the task was released to participants, the rubric for relevance judgments was not yet set, although the coordinators had expected the scheme to mirror that of background linking. However, as the assessment plan was developed, it was clear that a wikification result could be poor for multiple reasons. Thus, the following two-level, four-part scheme was used: 1. is the anchor sensible, that is, does it break on word boundaries and seem reasonable in a lexical sense? Anchors that break in the middle of words or awkwardly in the middle of phrases are not sensible. 2. is the anchor useful, that is, would a link at that place in the document possible help the reader better understand the article? Converting a noun phrase that has no bearing on the main topic of the article would be sensible, but not useful. 3. does the target match the anchor? Is the target document what the reader would reasonably expect to be linked from that anchor? Linking the text Justin Bieber to the Wikipedia page for Barack Obama would not match, although the anchor seems sensible and is conceivably useful. 4. is the target helpful? Does the content in the linked article provide background information or context for understanding the topic document? This is essentially the relevance criterion in the background linking task.</p></li></ul></div><div id=Podcasts><h4><a>Podcasts</a></h4><ul><li><a>Segment Retrieval</a><p>The retrieval task was defined as the problem of finding relevant segments from the episodes for a set of search queries which were provided in traditional TREC topic format. The provided transcripts have word-level time-stamps on a granularity of 0.1s which allows retrieval systems to index the contents by time osets. A segment was defined to be a two-minute chunk starting on the minute; e.g. [0.0-119.9] seconds, [60-199.9] seconds, [120-139.9] seconds, etc. Segments overlap each other by one minute - any segment except for the first and last segment is covered by the preceding and following segments. The rationale for creating overlapping segments is to account for the case where a phrase or sentence is split across segment boundaries. This creates 3.4M segments in total from the document collection with an average word count of 340 70 per segment. Topics consist of a topic number, keyword query, a type label, and a description of the user s information need. Eight topics were given at the outset for the participants to practice on, and 50 topics were released as the test task. Topics were formulated in three types: topical, re-finding, and known item. Example topics are given in Figure 2.</p></li></ul><ul><li><a>Summarization Task</a><p>Given a podcast episode, its audio, and transcription, the task is to return a short text snippet which accurately conveys the content of the podcast. Returned summaries should be grammatical, standalone uerances of significantly shorter length than the input episode description, short enough to be quickly read on a smartphone screen. No ground truth summaries are provided; the closest proxies are the show and episode descriptions provided by the podcast creators. We observe that these descriptions vary widely in scope, and are not always intended to act as summaries of the episode content, reflecting the dierent genres represented in the sample and the dierent intentions of the creators for the descriptions. We filtered the descriptions to establish a subset that is more appropriate as a ground truth set compared to full set of descriptions. The filtering was done with three heuristics shown in Table 5. These filters overlap to some extent, and remove about a third of the entire set; the remaining 66,245 descriptions we call the Brass Set.</p></li></ul></div><div id="Precision Medicine"><h4><a>Precision Medicine</a></h4><ul><li><a>Task</a><p>The task this year was focused on finding information about a specific treatment that an oncologist might consider for a patient. Specifically, the task focused on identifying critical evidence for or against the treatment in the specific population represented by the type of cancer and genetic mutation(s) in the topic. The topic structure is close to the Evidence Based Medicine (EBM) PICO framework, where the problem/population (P) is the cancer and its mutations, the intervention (I) is the treatment, the comparison (C) is an alternative treatment, and the outcomes (O) are the endpoints of a cancer study, such as event-free survival, quality of life or time to progression. This framework was designed to find the most relevant scientific articles for an individual patient, specifically searching the scientific literature in PubMed. The EBM notion of relevance combines topical evidence with the strength of evidence. The task emulated this notion and required strong evidence for the treatment (whether positive or negative) to be ranked over weaker evidence. There are often many treatments for a particular type of cancer and particular genetic mutations, so a useful clinical decision support tool will help oncologists narrow the treatment decision to the one most likely to help the patient. This is why strong negative evidence is important: it helps eliminate the treatment so that a more efficacious treatment can be chosen instead. The idea is to provide oncologists with the evidence that best helps them make a decision when evaluating competing alternatives. The primary literature corpus is therefore a snapshot of MEDLINE abstracts (i.e., what is searchable through the PubMed interface). The same MEDLINE-baseline snapshot that was used for the 2019 track was used this year. Of course, this collection should not be used to provide real-time decision support in 2020, but it is large enough to support the goals of this evaluation. Specifically, this corpus is composed of 29,138,916 MEDLINE abstracts.</p></li></ul></div></div></section></div><div style=flex-grow:1></div><footer class="bg-gradient-light py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5"><div class=container-fluid><p class="text-muted small col-12 px-0 px-sm-3"><span style=float:left><a style=color:#6c757d href=/anthology/info/credits/>Credits</a></span>
<span style=float:right>©2022 <a style=color:#6c757d href=https://webis.de/>Webis Group</a>&nbsp;&nbsp;•&nbsp;&nbsp;<a href=https://github.com/ir-anthology/ir-anthology><svg width="16" height="16" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 1C5.03 1 1 5.03 1 10c0 3.98 2.58 7.35 6.16 8.54C7.61 18.62 7.77 18.34 7.77 18.11 7.77 17.9 7.76 17.33 7.76 16.58 5.26 17.12 4.73 15.37 4.73 15.37c-.41-1.04-1-1.32-1-1.32C2.91 13.5 3.79 13.5 3.79 13.5 4.69 13.56 5.17 14.43 5.17 14.43c.8 1.37 2.11.98 2.62.75C7.87 14.6 8.1 14.2 8.36 13.98c-2-.23-4.1-1-4.1-4.45C4.26 8.55 4.61 7.74 5.19 7.11 5.1 6.88 4.79 5.97 5.28 4.73c0 0 .76-.24 2.47.92C8.47 5.45 9.24 5.35 10 5.35S11.53 5.45 12.25 5.65c1.72-1.17 2.47-.92 2.47-.92C15.21 5.97 14.9 6.88 14.81 7.11c.58.63.92 1.43.92 2.42.0 3.46-2.1 4.22-4.11 4.44C11.94 14.25 12.23 14.8 12.23 15.64 12.23 16.84 12.22 17.81 12.22 18.11 12.22 18.35 12.38 18.63 12.84 18.54 16.42 17.35 19 13.98 19 10c0-4.97-4.03-9-9-9z" fill="#6c757d"/></svg>
</a>&nbsp;<a href=https://twitter.com/IRanthology><svg width="18" height="18" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M19 4.74C18.339 5.029 17.626 5.229 16.881 5.32 17.644 4.86 18.227 4.139 18.503 3.28 17.79 3.7 17.001 4.009 16.159 4.17 15.485 3.45 14.526 3 13.464 3 11.423 3 9.771 4.66 9.771 6.7 9.771 6.99 9.804 7.269 9.868 7.539c-3.073-.159-5.792-1.62-7.614-3.86C1.936 4.219 1.754 4.86 1.754 5.539c0 1.281.651 2.411 1.643 3.071C2.79 8.589 2.22 8.429 1.723 8.149V8.189c0 1.789 1.274 3.289 2.963 3.631C4.376 11.899 4.049 11.939 3.713 11.939 3.475 11.939 3.245 11.919 3.018 11.88c.472 1.469 1.834 2.539 3.451 2.569-1.264.98-2.857 1.57-4.587 1.57C1.583 16.019 1.29 16.009 1 15.969c1.635 1.05 3.576 1.66 5.662 1.66 6.792.0 10.508-5.629 10.508-10.5C17.17 6.969 17.166 6.809 17.157 6.649 17.879 6.129 18.504 5.478 19 4.74" fill="#6c757d"/></svg>
</a>&nbsp;&nbsp;•&nbsp;&nbsp;<a style=color:#6c757d href=https://webis.de/people.html>Contact</a>&nbsp;&nbsp;•&nbsp;&nbsp;<a style=color:#6c757d href=https://webis.de/impressum.html>Impressum / Terms / Privacy</a></span></p><br></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=/js/bootstrap.min.js></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>